<!DOCTYPE html><html class="default" lang="en" data-base="."><head><meta charset="utf-8"/><meta http-equiv="x-ua-compatible" content="IE=edge"/><title>@wllama/wllama</title><meta name="description" content="Documentation for @wllama/wllama"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="assets/style.css"/><link rel="stylesheet" href="assets/highlight.css"/><script defer src="assets/main.js"></script><script async src="assets/icons.js" id="tsd-icons-script"></script><script async src="assets/search.js" id="tsd-search-script"></script><script async src="assets/navigation.js" id="tsd-nav-script"></script></head><body><script>document.documentElement.dataset.theme = localStorage.getItem("tsd-theme") || "os";document.body.style.display="none";setTimeout(() => app?app.showPage():document.body.style.removeProperty("display"),500)</script><header class="tsd-page-toolbar"><div class="tsd-toolbar-contents container"><div class="table-cell" id="tsd-search"><div class="field"><label for="tsd-search-field" class="tsd-widget tsd-toolbar-icon search no-caption"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-search"></use></svg></label><input type="text" id="tsd-search-field" aria-label="Search"/></div><div class="field"><div id="tsd-toolbar-links"></div></div><ul class="results"><li class="state loading">Preparing search index...</li><li class="state failure">The search index is not available</li></ul><a href="index.html" class="title">@wllama/wllama</a></div><div class="table-cell" id="tsd-widgets"><a href="#" class="tsd-widget tsd-toolbar-icon menu no-caption" data-toggle="menu" aria-label="Menu"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-menu"></use></svg></a></div></div></header><div class="container container-main"><div class="col-content"><div class="tsd-page-title"><h1>@wllama/wllama</h1></div><div class="tsd-panel tsd-typography"><a id="wllama---wasm-binding-for-llamacpp" class="tsd-anchor"></a><h1 class="tsd-anchor-link">wllama - Wasm binding for llama.cpp<a href="#wllama---wasm-binding-for-llamacpp" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h1><p><img src="media/README_banner.png" alt=""></p>
<p>WebAssembly binding for <a href="https://github.com/ggerganov/llama.cpp" target="_blank" class="external">llama.cpp</a></p>
<p>👉 <a href="https://huggingface.co/spaces/ngxson/wllama" target="_blank" class="external">Try the demo app</a></p>
<p>📄 <a href="https://github.ngxson.com/wllama/docs/" target="_blank" class="external">Documentation</a></p>
<p>For changelog, please visit <a href="https://github.com/ngxson/wllama/releases" target="_blank" class="external">releases page</a></p>
<div class="tsd-alert tsd-alert-important"><div class="tsd-alert-title"><svg width="16" height="16" viewBox="0 0 16 16"><use href="assets/icons.svg#icon-alertImportant"></use></svg><span>Important</span></div><p><br>
Version 2.0 is released 👉 <a href="media/intro-v2.md">read more</a></p>
</div>
<p><img src="media/screenshot_0.png" alt=""></p>
<a id="features" class="tsd-anchor"></a><h2 class="tsd-anchor-link">Features<a href="#features" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h2><ul>
<li>Typescript support</li>
<li>Can run inference directly on browser (using <a href="https://emscripten.org/docs/porting/simd.html" target="_blank" class="external">WebAssembly SIMD</a>), no backend or GPU is needed!</li>
<li>No runtime dependency (see <a href="media/package.json">package.json</a>)</li>
<li>High-level API: completions, embeddings</li>
<li>Low-level API: (de)tokenize, KV cache control, sampling control,...</li>
<li>Ability to split the model into smaller files and load them in parallel (same as <code>split</code> and <code>cat</code>)</li>
<li>Auto switch between single-thread and multi-thread build based on browser support</li>
<li>Inference is done inside a worker, does not block UI render</li>
<li>Pre-built npm package <a href="https://www.npmjs.com/package/@wllama/wllama" target="_blank" class="external">@wllama/wllama</a></li>
</ul>
<p>Limitations:</p>
<ul>
<li>To enable multi-thread, you must add <code>Cross-Origin-Embedder-Policy</code> and <code>Cross-Origin-Opener-Policy</code> headers. See <a href="https://github.com/ffmpegwasm/ffmpeg.wasm/issues/106#issuecomment-913450724" target="_blank" class="external">this discussion</a> for more details.</li>
<li>No WebGPU support, but maybe possible in the future</li>
<li>Max file size is 2GB, due to <a href="https://stackoverflow.com/questions/17823225/do-arraybuffers-have-a-maximum-length" target="_blank" class="external">size restriction of ArrayBuffer</a>. If your model is bigger than 2GB, please follow the <strong>Split model</strong> section below.</li>
</ul>
<a id="code-demo-and-documentation" class="tsd-anchor"></a><h2 class="tsd-anchor-link">Code demo and documentation<a href="#code-demo-and-documentation" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h2><p>📄 <a href="https://github.ngxson.com/wllama/docs/" target="_blank" class="external">Documentation</a></p>
<p>Demo:</p>
<ul>
<li>Basic usages with completions and embeddings: <a href="https://github.ngxson.com/wllama/examples/basic/" target="_blank" class="external">https://github.ngxson.com/wllama/examples/basic/</a></li>
<li>Embedding and cosine distance: <a href="https://github.ngxson.com/wllama/examples/embeddings/" target="_blank" class="external">https://github.ngxson.com/wllama/examples/embeddings/</a></li>
<li>For more advanced example using low-level API, have a look at test file: <a href="media/wllama.test.ts">wllama.test.ts</a></li>
</ul>
<a id="how-to-use" class="tsd-anchor"></a><h2 class="tsd-anchor-link">How to use<a href="#how-to-use" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h2><a id="use-wllama-inside-react-typescript-project" class="tsd-anchor"></a><h3 class="tsd-anchor-link">Use Wllama inside React Typescript project<a href="#use-wllama-inside-react-typescript-project" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h3><p>Install it:</p>
<pre><code class="bash"><span class="hl-0">npm</span><span class="hl-1"> </span><span class="hl-2">i</span><span class="hl-1"> </span><span class="hl-2">@wllama/wllama</span>
</code><button type="button">Copy</button></pre>

<p>Then, import the module:</p>
<pre><code class="ts"><span class="hl-3">import</span><span class="hl-1"> { </span><span class="hl-4">Wllama</span><span class="hl-1"> } </span><span class="hl-3">from</span><span class="hl-1"> </span><span class="hl-2">&#39;@wllama/wllama&#39;</span><span class="hl-1">;</span><br/><span class="hl-5">let</span><span class="hl-1"> </span><span class="hl-4">wllamaInstance</span><span class="hl-1"> = </span><span class="hl-5">new</span><span class="hl-1"> </span><span class="hl-0">Wllama</span><span class="hl-1">(</span><span class="hl-6">WLLAMA_CONFIG_PATHS</span><span class="hl-1">, ...);</span><br/><span class="hl-7">// (the rest is the same with earlier example)</span>
</code><button type="button">Copy</button></pre>

<p>For complete code example, see <a href="media/wllama.context.tsx">examples/main/src/utils/wllama.context.tsx</a></p>
<p>NOTE: this example only covers completions usage. For embeddings, please see <a href="media/index.html">examples/embeddings/index.html</a></p>
<a id="prepare-your-model" class="tsd-anchor"></a><h3 class="tsd-anchor-link">Prepare your model<a href="#prepare-your-model" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h3><ul>
<li>It is recommended to split the model into <strong>chunks of maximum 512MB</strong>. This will result in slightly faster download speed (because multiple splits can be downloaded in parallel), and also prevent some out-of-memory issues.<br>
See the &quot;Split model&quot; section below for more details.</li>
<li>It is recommended to use quantized Q4, Q5 or Q6 for balance among performance, file size and quality. Using IQ (with imatrix) is <strong>not</strong> recommended, may result in slow inference and low quality.</li>
</ul>
<a id="simple-usage-with-es6-module" class="tsd-anchor"></a><h3 class="tsd-anchor-link">Simple usage with ES6 module<a href="#simple-usage-with-es6-module" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h3><p>For complete code, see <a href="media/index-1.html">examples/basic/index.html</a></p>
<pre><code class="javascript"><span class="hl-3">import</span><span class="hl-1"> { </span><span class="hl-4">Wllama</span><span class="hl-1"> } </span><span class="hl-3">from</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/index.js&#39;</span><span class="hl-1">;</span><br/><br/><span class="hl-1">(</span><span class="hl-5">async</span><span class="hl-1"> () </span><span class="hl-5">=&gt;</span><span class="hl-1"> {</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">CONFIG_PATHS</span><span class="hl-1"> = {</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;single-thread/wllama.wasm&#39;</span><span class="hl-4">:</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/single-thread/wllama.wasm&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;multi-thread/wllama.wasm&#39;</span><span class="hl-4"> :</span><span class="hl-1"> </span><span class="hl-2">&#39;./esm/multi-thread/wllama.wasm&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">  };</span><br/><span class="hl-1">  </span><span class="hl-7">// Automatically switch between single-thread and multi-thread version based on browser support</span><br/><span class="hl-1">  </span><span class="hl-7">// If you want to enforce single-thread, add { &quot;n_threads&quot;: 1 } to LoadModelConfig</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">wllama</span><span class="hl-1"> = </span><span class="hl-5">new</span><span class="hl-1"> </span><span class="hl-0">Wllama</span><span class="hl-1">(</span><span class="hl-6">CONFIG_PATHS</span><span class="hl-1">);</span><br/><span class="hl-1">  </span><span class="hl-7">// Define a function for tracking the model download progress</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-0">progressCallback</span><span class="hl-1"> =  ({ </span><span class="hl-4">loaded</span><span class="hl-1">, </span><span class="hl-4">total</span><span class="hl-1"> }) </span><span class="hl-5">=&gt;</span><span class="hl-1"> {</span><br/><span class="hl-1">    </span><span class="hl-7">// Calculate the progress as a percentage</span><br/><span class="hl-1">    </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">progressPercentage</span><span class="hl-1"> = </span><span class="hl-4">Math</span><span class="hl-1">.</span><span class="hl-0">round</span><span class="hl-1">((</span><span class="hl-4">loaded</span><span class="hl-1"> / </span><span class="hl-4">total</span><span class="hl-1">) * </span><span class="hl-8">100</span><span class="hl-1">);</span><br/><span class="hl-1">    </span><span class="hl-7">// Log the progress in a user-friendly format</span><br/><span class="hl-1">    </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">log</span><span class="hl-1">(</span><span class="hl-2">`Downloading... </span><span class="hl-5">${</span><span class="hl-4">progressPercentage</span><span class="hl-5">}</span><span class="hl-2">%`</span><span class="hl-1">);</span><br/><span class="hl-1">  };</span><br/><span class="hl-1">  </span><span class="hl-7">// Load GGUF from Hugging Face hub</span><br/><span class="hl-1">  </span><span class="hl-7">// (alternatively, you can use loadModelFromUrl if the model is not from HF hub)</span><br/><span class="hl-1">  </span><span class="hl-3">await</span><span class="hl-1"> </span><span class="hl-4">wllama</span><span class="hl-1">.</span><span class="hl-0">loadModelFromHF</span><span class="hl-1">(</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;ggml-org/models&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">&#39;tinyllamas/stories260K.gguf&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    {</span><br/><span class="hl-1">      </span><span class="hl-4">progressCallback</span><span class="hl-1">,</span><br/><span class="hl-1">    }</span><br/><span class="hl-1">  );</span><br/><span class="hl-1">  </span><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">outputText</span><span class="hl-1"> = </span><span class="hl-3">await</span><span class="hl-1"> </span><span class="hl-4">wllama</span><span class="hl-1">.</span><span class="hl-0">createCompletion</span><span class="hl-1">(</span><span class="hl-4">elemInput</span><span class="hl-1">.</span><span class="hl-4">value</span><span class="hl-1">, {</span><br/><span class="hl-1">    </span><span class="hl-4">nPredict:</span><span class="hl-1"> </span><span class="hl-8">50</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-4">sampling:</span><span class="hl-1"> {</span><br/><span class="hl-1">      </span><span class="hl-4">temp:</span><span class="hl-1"> </span><span class="hl-8">0.5</span><span class="hl-1">,</span><br/><span class="hl-1">      </span><span class="hl-4">top_k:</span><span class="hl-1"> </span><span class="hl-8">40</span><span class="hl-1">,</span><br/><span class="hl-1">      </span><span class="hl-4">top_p:</span><span class="hl-1"> </span><span class="hl-8">0.9</span><span class="hl-1">,</span><br/><span class="hl-1">    },</span><br/><span class="hl-1">  });</span><br/><span class="hl-1">  </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">log</span><span class="hl-1">(</span><span class="hl-4">outputText</span><span class="hl-1">);</span><br/><span class="hl-1">})();</span>
</code><button type="button">Copy</button></pre>

<p>Alternatively, you can use the <code>*.wasm</code> files from CDN:</p>
<pre><code class="js"><span class="hl-3">import</span><span class="hl-1"> </span><span class="hl-4">WasmFromCDN</span><span class="hl-1"> </span><span class="hl-3">from</span><span class="hl-1"> </span><span class="hl-2">&#39;@wllama/wllama/esm/wasm-from-cdn.js&#39;</span><span class="hl-1">;</span><br/><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">wllama</span><span class="hl-1"> = </span><span class="hl-5">new</span><span class="hl-1"> </span><span class="hl-0">Wllama</span><span class="hl-1">(</span><span class="hl-4">WasmFromCDN</span><span class="hl-1">);</span><br/><span class="hl-7">// NOTE: this is not recommended, only use when you can&#39;t embed wasm files in your project</span>
</code><button type="button">Copy</button></pre>

<a id="split-model" class="tsd-anchor"></a><h3 class="tsd-anchor-link">Split model<a href="#split-model" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h3><p>Cases where we want to split the model:</p>
<ul>
<li>Due to <a href="https://stackoverflow.com/questions/17823225/do-arraybuffers-have-a-maximum-length" target="_blank" class="external">size restriction of ArrayBuffer</a>, the size limitation of a file is 2GB. If your model is bigger than 2GB, you can split the model into small files.</li>
<li>Even with a small model, splitting into chunks allows the browser to download multiple chunks in parallel, thus making the download process a bit faster.</li>
</ul>
<p>We use <code>llama-gguf-split</code> to split a big gguf file into smaller files. You can download the pre-built binary via <a href="https://github.com/ggerganov/llama.cpp/releases" target="_blank" class="external">llama.cpp release page</a>:</p>
<pre><code class="bash"><span class="hl-7"># Split the model into chunks of 512 Megabytes</span><br/><span class="hl-0">./llama-gguf-split</span><span class="hl-1"> </span><span class="hl-5">--split-max-size</span><span class="hl-1"> </span><span class="hl-2">512M</span><span class="hl-1"> </span><span class="hl-2">./my_model.gguf</span><span class="hl-1"> </span><span class="hl-2">./my_model</span>
</code><button type="button">Copy</button></pre>

<p>This will output files ending with <code>-00001-of-00003.gguf</code>, <code>-00002-of-00003.gguf</code>, and so on.</p>
<p>You can then pass to <code>loadModelFromUrl</code> or <code>loadModelFromHF</code> the URL of the first file and it will automatically load all the chunks:</p>
<pre><code class="js"><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">wllama</span><span class="hl-1"> = </span><span class="hl-5">new</span><span class="hl-1"> </span><span class="hl-0">Wllama</span><span class="hl-1">(</span><span class="hl-6">CONFIG_PATHS</span><span class="hl-1">, {</span><br/><span class="hl-1">  </span><span class="hl-4">parallelDownloads:</span><span class="hl-1"> </span><span class="hl-8">5</span><span class="hl-1">, </span><span class="hl-7">// optional: maximum files to download in parallel (default: 3)</span><br/><span class="hl-1">});</span><br/><span class="hl-3">await</span><span class="hl-1"> </span><span class="hl-4">wllama</span><span class="hl-1">.</span><span class="hl-0">loadModelFromHF</span><span class="hl-1">(</span><br/><span class="hl-1">  </span><span class="hl-2">&#39;ngxson/tinyllama_split_test&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">  </span><span class="hl-2">&#39;stories15M-q8_0-00001-of-00003.gguf&#39;</span><br/><span class="hl-1">);</span>
</code><button type="button">Copy</button></pre>

<a id="custom-logger-suppress-debug-messages" class="tsd-anchor"></a><h3 class="tsd-anchor-link">Custom logger (suppress debug messages)<a href="#custom-logger-suppress-debug-messages" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h3><p>When initializing Wllama, you can pass a custom logger to Wllama.</p>
<p>Example 1: Suppress debug message</p>
<pre><code class="js"><span class="hl-3">import</span><span class="hl-1"> { </span><span class="hl-4">Wllama</span><span class="hl-1">, </span><span class="hl-4">LoggerWithoutDebug</span><span class="hl-1"> } </span><span class="hl-3">from</span><span class="hl-1"> </span><span class="hl-2">&#39;@wllama/wllama&#39;</span><span class="hl-1">;</span><br/><br/><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">wllama</span><span class="hl-1"> = </span><span class="hl-5">new</span><span class="hl-1"> </span><span class="hl-0">Wllama</span><span class="hl-1">(</span><span class="hl-4">pathConfig</span><span class="hl-1">, {</span><br/><span class="hl-1">  </span><span class="hl-7">// LoggerWithoutDebug is predefined inside wllama</span><br/><span class="hl-1">  </span><span class="hl-4">logger:</span><span class="hl-1"> </span><span class="hl-4">LoggerWithoutDebug</span><span class="hl-1">,</span><br/><span class="hl-1">});</span>
</code><button type="button">Copy</button></pre>

<p>Example 2: Add emoji prefix to log messages</p>
<pre><code class="js"><span class="hl-5">const</span><span class="hl-1"> </span><span class="hl-6">wllama</span><span class="hl-1"> = </span><span class="hl-5">new</span><span class="hl-1"> </span><span class="hl-0">Wllama</span><span class="hl-1">(</span><span class="hl-4">pathConfig</span><span class="hl-1">, {</span><br/><span class="hl-1">  </span><span class="hl-4">logger:</span><span class="hl-1"> {</span><br/><span class="hl-1">    </span><span class="hl-0">debug</span><span class="hl-4">:</span><span class="hl-1"> (...</span><span class="hl-4">args</span><span class="hl-1">) </span><span class="hl-5">=&gt;</span><span class="hl-1"> </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">debug</span><span class="hl-1">(</span><span class="hl-2">&#39;🔧&#39;</span><span class="hl-1">, ...</span><span class="hl-4">args</span><span class="hl-1">),</span><br/><span class="hl-1">    </span><span class="hl-0">log</span><span class="hl-4">:</span><span class="hl-1"> (...</span><span class="hl-4">args</span><span class="hl-1">) </span><span class="hl-5">=&gt;</span><span class="hl-1"> </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">log</span><span class="hl-1">(</span><span class="hl-2">&#39;ℹ️&#39;</span><span class="hl-1">, ...</span><span class="hl-4">args</span><span class="hl-1">),</span><br/><span class="hl-1">    </span><span class="hl-0">warn</span><span class="hl-4">:</span><span class="hl-1"> (...</span><span class="hl-4">args</span><span class="hl-1">) </span><span class="hl-5">=&gt;</span><span class="hl-1"> </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">warn</span><span class="hl-1">(</span><span class="hl-2">&#39;⚠️&#39;</span><span class="hl-1">, ...</span><span class="hl-4">args</span><span class="hl-1">),</span><br/><span class="hl-1">    </span><span class="hl-0">error</span><span class="hl-4">:</span><span class="hl-1"> (...</span><span class="hl-4">args</span><span class="hl-1">) </span><span class="hl-5">=&gt;</span><span class="hl-1"> </span><span class="hl-4">console</span><span class="hl-1">.</span><span class="hl-0">error</span><span class="hl-1">(</span><span class="hl-2">&#39;☠️&#39;</span><span class="hl-1">, ...</span><span class="hl-4">args</span><span class="hl-1">),</span><br/><span class="hl-1">  },</span><br/><span class="hl-1">});</span>
</code><button type="button">Copy</button></pre>

<a id="how-to-compile-the-binary-yourself" class="tsd-anchor"></a><h2 class="tsd-anchor-link">How to compile the binary yourself<a href="#how-to-compile-the-binary-yourself" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h2><p>This repository already come with pre-built binary from llama.cpp source code. However, in some cases you may want to compile it yourself:</p>
<ul>
<li>You don't trust the pre-built one.</li>
<li>You want to try out latest - bleeding-edge changes from upstream llama.cpp source code.</li>
</ul>
<p>You can use the commands below to compile it yourself:</p>
<pre><code class="shell"><span class="hl-7"># /!\ IMPORTANT: Require having docker compose installed</span><br/><br/><span class="hl-7"># Clone the repository with submodule</span><br/><span class="hl-0">git</span><span class="hl-1"> </span><span class="hl-2">clone</span><span class="hl-1"> </span><span class="hl-5">--recurse-submodules</span><span class="hl-1"> </span><span class="hl-2">https://github.com/ngxson/wllama.git</span><br/><span class="hl-0">cd</span><span class="hl-1"> </span><span class="hl-2">wllama</span><br/><br/><span class="hl-7"># Optionally, you can run this command to update llama.cpp to latest upstream version (bleeding-edge, use with your own risk!)</span><br/><span class="hl-7"># git submodule update --remote --merge</span><br/><br/><span class="hl-7"># Install the required modules</span><br/><span class="hl-0">npm</span><span class="hl-1"> </span><span class="hl-2">i</span><br/><br/><span class="hl-7"># Firstly, build llama.cpp into wasm</span><br/><span class="hl-0">npm</span><span class="hl-1"> </span><span class="hl-2">run</span><span class="hl-1"> </span><span class="hl-2">build:wasm</span><br/><span class="hl-7"># Then, build ES module</span><br/><span class="hl-0">npm</span><span class="hl-1"> </span><span class="hl-2">run</span><span class="hl-1"> </span><span class="hl-2">build</span>
</code><button type="button">Copy</button></pre>

<a id="todo" class="tsd-anchor"></a><h2 class="tsd-anchor-link">TODO<a href="#todo" aria-label="Permalink" class="tsd-anchor-icon"><svg viewBox="0 0 24 24"><use href="assets/icons.svg#icon-anchor"></use></svg></a></h2><ul>
<li>Add support for LoRA adapter</li>
<li>Support GPU inference via WebGL</li>
<li>Support multi-sequences: knowing the resource limitation when using WASM, I don't think having multi-sequences is a good idea</li>
<li>Multi-modal: Waiting for refactoring LLaVA implementation from llama.cpp</li>
</ul>
</div></div><div class="col-sidebar"><div class="page-menu"><div class="tsd-navigation settings"><details class="tsd-accordion"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>Settings</h3></summary><div class="tsd-accordion-details"><div class="tsd-filter-visibility"><span class="settings-label">Member Visibility</span><ul id="tsd-filter-options"><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-protected" name="protected"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Protected</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-inherited" name="inherited" checked/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Inherited</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-external" name="external"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>External</span></label></li></ul></div><div class="tsd-theme-toggle"><label class="settings-label" for="tsd-theme">Theme</label><select id="tsd-theme"><option value="os">OS</option><option value="light">Light</option><option value="dark">Dark</option></select></div></div></details></div><details open class="tsd-accordion tsd-page-navigation"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>On This Page</h3></summary><div class="tsd-accordion-details"><a href="#wllama---wasm-binding-for-llamacpp"><span>wllama -<wbr/> <wbr/>Wasm binding for llama.cpp</span></a><ul><li><a href="#features"><span>Features</span></a></li><li><a href="#code-demo-and-documentation"><span>Code demo and documentation</span></a></li><li><a href="#how-to-use"><span>How to use</span></a></li><li><ul><li><a href="#use-wllama-inside-react-typescript-project"><span>Use <wbr/>Wllama inside <wbr/>React <wbr/>Typescript project</span></a></li><li><a href="#prepare-your-model"><span>Prepare your model</span></a></li><li><a href="#simple-usage-with-es6-module"><span>Simple usage with ES6 module</span></a></li><li><a href="#split-model"><span>Split model</span></a></li><li><a href="#custom-logger-suppress-debug-messages"><span>Custom logger (suppress debug messages)</span></a></li></ul></li><li><a href="#how-to-compile-the-binary-yourself"><span>How to compile the binary yourself</span></a></li><li><a href="#todo"><span>TODO</span></a></li></ul></div></details></div><div class="site-menu"><nav class="tsd-navigation"><a href="modules.html" class="current">@wllama/wllama</a><ul class="tsd-small-nested-navigation" id="tsd-nav-container"><li>Loading...</li></ul></nav></div></div></div><footer><p class="tsd-generator">Generated using <a href="https://typedoc.org/" target="_blank">TypeDoc</a></p></footer><div class="overlay"></div></body></html>
